---
title: "Write Up - Christian Smith"
output:
  html_document: default
  pdf_document: default
date: '2022-08-07'
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Explanation

As I was working through this project and code I found myself doing quite a bit of cleaning. Whether it was me having to go back to include another variable, or adding in a new variable to account for time periods etc., I spent a fair amount of time wrangling the data. There was a lot of back tracking and rerunning the code in order to start to get it to a comfortable place where I felt the data was in a good spot for cleaning after working out many of the kinks. After splitting out the data by pre college scorecard release and post college scorecard release, I also did a split for low and high earning schools, the way I did this was taking the median of the median income, and the mean of the median income, and since the two numbers were relatively close at 37,800 for the median and 39,036 for the mean. Since they were close and wanted to account for both, I took the difference if the two and set the split for high and low at 38,400. After that, I wanted to do a basic quick view of the differences in median income and mean median income just as a way to get a sense of the changes between the two before I started to get into the meat of running models and regressions. What surprised me was that it didn't show much of a change at all from pre and post in terms of median income 10 years after graduation. In fact it actually decreased from afterwards, which I would not have expected from the initial question and simply from intuition. So I started to think about reasons this might be in order to get variables for them or to be able to measure them. I thought about the differences in degrees and figured that it might also have a large part to do with median income not changing as the type of degree you get will dictate the industry you go into after graduation (for the most part, there are obviously some people who make a pivot). But then I thought about how the effects from that should balance out either way, and the question we want to look into is by school not my degree. So I thought about cost as well, knowing that the cost of the college also has an impact on whether people go to that school based on family wealth/in state opportunities etc. Obviously if Harvard or Yale were as cheap as in state schools then a lot more people would go, but again, the question we are trying to answer is whether the scorecard had an effect on people going to schools with higher median incomes, and being able to afford a certain school does not change. So when I went back to the data I figured there were a couple ways to measure it, we could look at the type of school as listed by the "CONTROL" variable, we could take a look at a certain split out between high and low earnings people and see if there are differences there based on control. We also could look at the interaction between these variables and see if those account for more explanation of the numbers. So as I started thinking about that, I played around with a lot of combinations based on the pre and post scorecard release, getting an idea of the relationship between certain variables in high and low income people, both before and after the scorecard. I also played with some controls and a couple interactions, but most of them weren't giving me a great r squared even with significant level p values. After going through all of these and not wanting to use them, I decided to take a look at a few basic graphs to see the trends over time, and what I saw was that Pre scorecard data remained fairly steady across time in regards to median income, but Post scorecard data showed a steady decline over time in terms of median income. Based on these plots below, I decided to do some comparisons of models Pre and Post scorecard now that I had a better sense of direction and where I should focus the analysis to go. Now I also did some analysis on those same variables but used the High and Low earning schools as well, but since the low earning schools had a pretty consistent median income over time, it wouldn't impact the trends we saw, so I just decided to use to whole Pre and Post sets. I also am aware about this not really showing a whole lot graphically, but I felt that because it brought me the right place and took me to the final models, it was worth including, even though a lot of my analysis and takeaways were not exactly what I needed to be looking at. 

```{r}
# Libraries
library(ggplot2)
library(tidyverse)
library(huxtable)
library(readr)
library(purrr)
library(lubridate)
library(estimatr)
library(fixest)
library(jtools)
library(ggstance)
library(ggplot2)

#Data Sets
trends <- list.files(path = "Data", pattern = 'trends_up_to_')
gtrend <- map_df(trends, read_csv)
college_score <- read_csv("Data/Most+Recent+Cohorts+(Scorecard+Elements).csv")

rm_dup <- read_csv("Data/id_name_link.csv") %>% rename(OPEID = opeid) %>%
  rename(UNITID = unitid) %>% distinct(schname, .keep_all = TRUE)
gtrend <- gtrend %>% 
  mutate(Day = substr(monthorweek, 1, 10)) %>%
  mutate(Day = as.Date(Day))

id <- merge(x = rm_dup, y = college_score, by = c('UNITID', 'OPEID'), all.x = TRUE)
combine <- merge(x = id, y = gtrend, by = 'schname', all.x = TRUE)
teat <- combine %>% na.omit()
teat <- teat %>%
  rename(median_earn = 'md_earn_wne_p10-REPORTED-EARNINGS') %>%
  filter(median_earn != 'NULL') %>%
  mutate(median_earn = as.numeric(median_earn))
gray <- teat %>%
  group_by(keynum) %>%
  summarise(sdindex = (index - mean(index)) / sd(index), schname, CONTROL, 
            keyword, monthorweek, keynum, sdindex, median_earn, Day)
teat <- teat %>%
  select('UNITID', 'OPEID', 'schname', 'CITY', 'STABBR', 'PREDDEG', 'LOCALE', 'keyword', 'monthorweek', 'keynum', 'index', 'median_earn', 'Day')
gray2 <- na.omit(gray)
median <- median(gray2$median_earn)
mean <- mean(gray2$median_earn)
split <- 38400
atlas <- gray2
atlas$High <- ifelse(atlas$median_earn >= split, 1, 0)

Pre <- atlas %>% 
  filter(Day < '2015-09-01')
Post <- atlas %>%
  filter(Day >= '2015-09-01')

PostHigh <- atlas %>%
  filter(High == 1) %>%
  filter(Day >= '2015-09-01')
PostLow <- atlas %>%
  filter(High == 0) %>%
  filter(Day >= '2015-09-01')
PreHigh <- atlas %>%
  filter(High == 1) %>%
  filter(Day < '2015-09-01')
PreLow <- atlas %>%
  filter(High == 0) %>%
  filter(Day < '2015-09-01')


ggplot(data = Pre, aes(Day, median_earn)) +
  geom_smooth() + ggtitle("Pre Sept 2015")

ggplot(data = Post, aes(Day, median_earn)) +
  geom_smooth() + ggtitle("Post Sept 2015")
```

So after I got some better direction on how to handle the regression models I wanted to run, I began messing around with some models to see where the key relationships might be, and some again like before were significant, but the r squared was pretty low so I figured I would try a couple interactions and additions to see where I could possibly keep the significance level of the variables, but also get a better r squared, which as mentioned in the lectures shouldn't be super focused on, but I figured I would try and see what I could get and it ended up working out pretty good. The models below are the ones I focused mainly on. Of the three comparisons I looked at with etables, two of the three had much higher r squared which I was originally looking for. Those models were the two 'another' models comparison and the 't77' & 'g2' comparisons. Based on these two I looked at the wald results for them to see if there was any more insight there that I could grab for certain variables, which again it showed high p value/significance at a high confidence level on the varaibles I took a look at. Even though I did some playing around with the variables to see if any were indeed equal to zero, I only sw high values as well as a very significant p value for the tests which shows that all of these variables I am using are significant and should stay in the model. So after all this I decided that I felt the best model to include would be the one that has 'sdindex' as well as the interaction between type of school and high median earning schools. I felt that this interaction was worth including as it would give us a sense of how the type of school and weather it's a high earning school should be included. Now during my analysis what threw me off a bit is that when I just simply did a model based on sdindex and whether the school was High earning or not I got a much different picture. Which is why I started to do some more analysis with sdindex as the dependent variable. I will explain my findings down below.


```{r}

t7 <- feols(median_earn ~ sdindex + CONTROL, data = Pre)
t77 <- feols(median_earn ~ sdindex + High, data = Pre)
g24  <- feols(median_earn ~ sdindex + CONTROL, data = Post)
g2 <- feols(median_earn ~ sdindex + High, data = Post)

another <- feols(median_earn ~ sdindex + CONTROL * High, data = Pre)
another2 <- feols(median_earn ~ sdindex + CONTROL * High, data = Post)

t71 <- feols(median_earn ~ sdindex, data = PreLow)
g4 <- feols(median_earn ~ sdindex, data = PostLow)

anotha <- feols(sdindex ~ CONTROL + High, data = Pre)
anotha2 <- feols(sdindex ~ CONTROL + High, data = Post)

t1 <- feols(sdindex ~ High, data = Pre)
t2 <- feols(sdindex ~ High, data = Post)

etable(t1, t2)
etable(another, another2)
etable(anotha, anotha2)

```

So after going through that rabbit hole and going back and forth with certain new variables and realizing the questions was looking at impact of search term, I realized I needed to adjust some of my information and formulas. So what I did then was to use the best model I had for sdindex as the dependent variable that includes 'CONTROL' & 'High' as the independent variables. What this tells me is that the High earning schools before the scorecard was released were increasing the sdindex that measures searches by .1756 and after the scorecard was released, the high earning schools increased the sdindex by .2487. Now the intercept did get bigger as it is negative, so I also wanted to calculate the total impact including the error term which showed pre scorecard, High Earning schools increased sdindex by .0389 and after the scorecard was released, higher earning schools increased the sdindex by .0451. But the problem is that the 'CONTROL' is going to impact it as well, so I will do a calculation for all three levels of school that we have. Reminder that 1 = public, 2 = private non profit, and 3 = private for profit. So comparing before and after scorecard release, for  public we get .1003 and -.0165 respectively, for private non profit we get .1617 and -.0781 respectively, and for private for profit we get .2231 and -.1397 respectively. So based on the error term in the basic model canceling out any positive effect that was shown after the scorecard released, and when we show the calculations for the impact to the sdindex, it clearly shows that high earning schools are being searched less, which means that post scorecard release, high earning schools have a negative relationship with sdindex that measures search popularity. I think there are a few ways to explain how this might happen with people already knowing about the school so they don't need to search for them, or it just confirmed that people now know it's a high earning school so they don't feel the need to search as much. But regardless of any of that, the data shows that the release of the scorecard reduced the student interest among high earning schools.

